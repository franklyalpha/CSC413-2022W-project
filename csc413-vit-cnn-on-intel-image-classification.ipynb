{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install timm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-17T22:08:35.805946Z","iopub.execute_input":"2022-04-17T22:08:35.806354Z","iopub.status.idle":"2022-04-17T22:08:44.891187Z","shell.execute_reply.started":"2022-04-17T22:08:35.806301Z","shell.execute_reply":"2022-04-17T22:08:44.889995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms  \nimport torchvision\nimport os\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom torch.utils.data import random_split\n\nimport cv2\nimport pandas as pd\nimport torchvision.transforms as transforms \nfrom torchvision.transforms import ToTensor,Normalize, RandomHorizontalFlip, Resize\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.autograd import Variable\n\nimport timm\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:08:49.725104Z","iopub.execute_input":"2022-04-17T22:08:49.725475Z","iopub.status.idle":"2022-04-17T22:08:52.814973Z","shell.execute_reply.started":"2022-04-17T22:08:49.725434Z","shell.execute_reply":"2022-04-17T22:08:52.813777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"below will load data and give transformer settings","metadata":{}},{"cell_type":"code","source":"data_dir_Train = \"/kaggle/input/intel-image-classification/seg_train\"\ndata_dir_Test = \"/kaggle/input/intel-image-classification/seg_test\"\ndata_dir_pred = \"/kaggle/input/intel-image-classification/seg_pred/seg_pred\"\n\ntrain_dir = data_dir_Train + \"/seg_train\"\n\n\n\n\nCNN_transforms = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.RandomHorizontalFlip(p=0.5), # randomly flip and rotate\n    transforms.ColorJitter(0.3,0.4,0.4,0.2),\n    transforms.ToTensor(),\n    transforms.Normalize((0.425, 0.415, 0.405), (0.205, 0.205, 0.205))\n])\n\nattention_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(p=0.3), # randomly flip and rotate\n#     transforms.RandomVerticalFlip(p=0.5),\n#     transforms.ColorJitter(0.3,0.4,0.4,0.2),\n    transforms.ToTensor(),\n    transforms.Normalize((0.425, 0.415, 0.405), (0.205, 0.205, 0.205))\n#     transforms.Normalize((0.4951, 0.4982, 0.4979), (0.2482, 0.2467, 0.2807))\n])\n\ntrain = torchvision.datasets.ImageFolder(train_dir, transform=attention_transform)\n\nval_size = int(len(train) * 0.2)\nnull_size = int(len(train) * 0.2)\ntrain_size = len(train) - val_size\n\ntrain_ds, val_ds= random_split(train, [train_size, val_size])","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:09:00.992322Z","iopub.execute_input":"2022-04-17T22:09:00.99266Z","iopub.status.idle":"2022-04-17T22:09:13.167214Z","shell.execute_reply.started":"2022-04-17T22:09:00.992622Z","shell.execute_reply":"2022-04-17T22:09:13.166225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below is code for checking whether enlarging an image is valid. \n\nThe method demonstrated is a typical way of loading an image to display. ","metadata":{}},{"cell_type":"code","source":"from torchvision.io import read_image\nimg_path = \"../input/intel-image-classification/seg_pred/seg_pred/10004.jpg\"\ntest_image = Image.open(img_path).convert(\"RGB\")\ntest_image.show()\n# test_image_2 = attention_transform(test_image)\n# test_image_2\n# # # test_image_2\n# tensor_image_2","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:09:21.793938Z","iopub.execute_input":"2022-04-17T22:09:21.794337Z","iopub.status.idle":"2022-04-17T22:09:21.84804Z","shell.execute_reply.started":"2022-04-17T22:09:21.794299Z","shell.execute_reply":"2022-04-17T22:09:21.847055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\nvalid_loader = DataLoader(val_ds, batch_size=128, num_workers=2, pin_memory=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:09:27.713364Z","iopub.execute_input":"2022-04-17T22:09:27.713688Z","iopub.status.idle":"2022-04-17T22:09:27.719685Z","shell.execute_reply.started":"2022-04-17T22:09:27.713655Z","shell.execute_reply":"2022-04-17T22:09:27.718847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"below gives device settings","metadata":{}},{"cell_type":"code","source":"device_use =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:09:35.62527Z","iopub.execute_input":"2022-04-17T22:09:35.626196Z","iopub.status.idle":"2022-04-17T22:09:35.630715Z","shell.execute_reply.started":"2022-04-17T22:09:35.626155Z","shell.execute_reply":"2022-04-17T22:09:35.629952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Following will be setup for models","metadata":{}},{"cell_type":"markdown","source":"below is the code for training loops","metadata":{}},{"cell_type":"code","source":"timm.list_models()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # code below is for revealing the structure of model\n\n# sample_model = timm.create_model('vit_base_patch16_224', num_classes=6)\n\n# sample_model\n# torch.manual_seed(42)\nmodel = timm.create_model('vit_base_patch16_224', pretrained=False)\nmodel.head = nn.Linear(model.head.in_features, 6)\n# model.load_state_dict(torch.load('../input/model-states/vit_normal_model_state.pt'))\n\n# below chunk is for loading state dict for CPU\nmodel.load_state_dict(torch.load('../input/model-states/VIT_model_state.pt', map_location=torch.device('cpu')))\n\n\nmodel.to(device_use)\nmodel","metadata":{"execution":{"iopub.status.busy":"2022-04-16T15:05:00.426801Z","iopub.execute_input":"2022-04-16T15:05:00.427227Z","iopub.status.idle":"2022-04-16T15:05:02.271532Z","shell.execute_reply.started":"2022-04-16T15:05:00.427193Z","shell.execute_reply":"2022-04-16T15:05:02.27095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# below loads convolution NN\n\nmodel = torchvision.models.wide_resnet50_2(pretrained=False)\n\n# for param in model.parameters():\n#     param.required_grad = False\n\n\nnum_ftrt = model.fc.in_features\n\nmodel.fc = nn.Linear(num_ftrt,6)\nmodel.load_state_dict(torch.load('../input/model-states/cnn_model_state.pt', map_location=torch.device('cpu')))\nmodel.to(device_use)\nmodel","metadata":{"execution":{"iopub.status.busy":"2022-04-16T15:04:53.41214Z","iopub.execute_input":"2022-04-16T15:04:53.412462Z","iopub.status.idle":"2022-04-16T15:04:57.233883Z","shell.execute_reply.started":"2022-04-16T15:04:53.412429Z","shell.execute_reply":"2022-04-16T15:04:57.233355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# configuration for optimizer\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\n\n# specify optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 5], gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T14:46:49.671078Z","iopub.execute_input":"2022-04-16T14:46:49.671892Z","iopub.status.idle":"2022-04-16T14:46:49.678982Z","shell.execute_reply.started":"2022-04-16T14:46:49.67184Z","shell.execute_reply":"2022-04-16T14:46:49.678089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load previously saved model for further training\nmodel.load_state_dict(torch.load('./model_state.pt'))\nmodel.to(device_use)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model):\n    valid_loss = 0.0\n    model.eval()\n    for batch_idx, (data, target) in enumerate(valid_loader):\n        \n        # move tensor to gpu\n        if torch.cuda.is_available():\n            data, target = data.to(device_use), target.to(device_use)\n        # forward pass: compute the validation predictions\n        output = model(data)\n        # calculate the loss\n        loss = criterion(output, target)\n        # update the validation loss \n        valid_loss += loss.item()*data.size(0)\n        if(batch_idx % 5 == 0):\n            print(batch_idx, valid_loss)\n    return valid_loss","metadata":{"execution":{"iopub.status.busy":"2022-04-15T16:02:00.201607Z","iopub.execute_input":"2022-04-15T16:02:00.201884Z","iopub.status.idle":"2022-04-15T16:02:00.211066Z","shell.execute_reply.started":"2022-04-15T16:02:00.201855Z","shell.execute_reply":"2022-04-15T16:02:00.210327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_loader.sampler)\nvalid_loss_min = np.Inf\nval_loss = []\ntn_loss = []","metadata":{"execution":{"iopub.status.busy":"2022-04-15T16:02:01.897819Z","iopub.execute_input":"2022-04-15T16:02:01.898316Z","iopub.status.idle":"2022-04-15T16:02:01.902236Z","shell.execute_reply.started":"2022-04-15T16:02:01.898279Z","shell.execute_reply":"2022-04-15T16:02:01.901466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Warning: running training code can be cumulative. Fortunately the saving option ensures over-trained model won't be saved. \nBy the way here tracks how many times below code has run. Thus trained for $x * recorded$ many epoches. \n\nNote: sometimes to overcome local minima, the model's validation loss will be high for at most 2-3 rounds(lr=1e-2). Thus should set an early stopping rate of 5 epoches for safety. \nAnd, patience is required, as 10 epoches of training can take 30 minutes. But the result is worthful. \n\ntraining consists of: \n20 epoches with lr=1e-2, overfit after 15 epoches\n\n10 epoches with lr=5e-4\n\nreached accuracy of 0.85","metadata":{}},{"cell_type":"code","source":"valid_loss_min","metadata":{"execution":{"iopub.status.busy":"2022-04-15T16:02:04.586744Z","iopub.execute_input":"2022-04-15T16:02:04.587256Z","iopub.status.idle":"2022-04-15T16:02:04.603698Z","shell.execute_reply.started":"2022-04-15T16:02:04.587209Z","shell.execute_reply":"2022-04-15T16:02:04.602905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# number of epochs for training set\nimport time\n\nepochs = 7\ntorch.cuda.empty_cache()\n# track change in validation loss\n\n\nfor epoch in range(epochs):\n\n    # keep track of training and validation loss\n    train_loss = 0.0\n    valid_loss = 0.0\n    time_start = time.time()\n    \n    # Train the model\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader): \n\n        # move tensor to gpu if cuda is available\n        if torch.cuda.is_available:\n            data, target = data.to(device_use), target.to(device_use)\n        # clear the gradiant of all optimizer variable\n        optimizer.zero_grad()\n        # forward pass: compute pradictions by passing inputs\n        output = model(data)\n        # calculate batch loss\n\n        loss = criterion(output, target)\n\n        # backward pass: compute gradiant of the loss with respect to the parameters\n        loss.backward()\n\n        # update parameters by optimizing single step\n        optimizer.step()\n\n        # update training loss\n        train_loss += loss.item()*data.size(0)\n        if(batch_idx % 10 == 0):\n            print(batch_idx, train_loss)\n    valid_loss = evaluate(model)\n    # calculate average loss\n    train_loss = train_loss/len(train_loader.sampler)\n    valid_loss = valid_loss/len(valid_loader.sampler)\n    val_loss.append(valid_loss)\n    tn_loss.append(train_loss)\n    # update learning rate\n    scheduler.step()\n    # Print the train and validation loss statistic\n    print('Epoch: {} \\t Training Loss: {:.3f} \\t Validation Loss: {:.3f}'.format(epoch, train_loss, valid_loss))\n    \n    # save model if validation loss decrease\n    if valid_loss <= valid_loss_min:\n        print(\"Validation loss decreased {:.4f}--->{:.4f}  Saving model...\".format(valid_loss_min, valid_loss))\n        # save current model\n        torch.save(model.state_dict(), 'model_state.pt')\n        valid_loss_min = valid_loss\n    print('Learning Rate ------------->{:.4f}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\n\n    time_end = time.time()\n    print(\"training time for epoch {} is: {}\".format(epoch, time_end - time_start))","metadata":{"execution":{"iopub.status.busy":"2022-04-15T16:02:07.4444Z","iopub.execute_input":"2022-04-15T16:02:07.444678Z","iopub.status.idle":"2022-04-15T16:15:48.17417Z","shell.execute_reply.started":"2022-04-15T16:02:07.444648Z","shell.execute_reply":"2022-04-15T16:15:48.173253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# below is the code segments for model evaluation. place them inside a function!!!\n\n\n@torch.no_grad()\ndef accuracy_examine():\n    valid_accuracy = 0.0\n    model.eval()\n\n    for batch_idx, (data, target) in enumerate(valid_loader):\n\n        # move tensors to GPU if CUDA is available\n        if device_use.type == \"cuda\":\n            data, target = data.cuda(), target.cuda()\n#         elif device_use.type == \"xla\":\n#             data = data.to(device_use, dtype=torch.float32)\n#             target = target.to(device_use, dtype=torch.int64)\n\n        with torch.no_grad():\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data)\n            # calculate the batch loss\n            \n            # Calculate Accuracy\n            accuracy = (output.argmax(dim=1) == target).float().mean()\n            # update average validation loss and accuracy\n            \n            valid_accuracy += accuracy\n#         if count % 20 == 0:\n#             print(count, valid_accuracy)\n    print(valid_accuracy)\n    print(len(valid_loader))\n    print(valid_accuracy / len(valid_loader))\n\naccuracy_examine()","metadata":{"execution":{"iopub.status.busy":"2022-04-15T16:15:56.336475Z","iopub.execute_input":"2022-04-15T16:15:56.337297Z","iopub.status.idle":"2022-04-15T16:16:05.619973Z","shell.execute_reply.started":"2022-04-15T16:15:56.337241Z","shell.execute_reply":"2022-04-15T16:16:05.619106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Starting from now, will implement saliency map based on trained model from above. \n\nGeneral idea is: extract the gradient of input from backward pass of evaluating an image(which requires forward pass). ","metadata":{}},{"cell_type":"code","source":"model.eval()\n\n\nimg_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# load single image\nsample_building = \"../input/intel-image-classification/seg_test/seg_test/buildings/20064.jpg\"\nsample_forest = \"../input/intel-image-classification/seg_test/seg_test/forest/20062.jpg\"\nsample_glacier = \"../input/intel-image-classification/seg_test/seg_test/glacier/20111.jpg\"\nsample_mountain = \"../input/intel-image-classification/seg_test/seg_test/mountain/20120.jpg\"\nsample_sea = \"../input/intel-image-classification/seg_test/seg_test/sea/20106.jpg\"\nsample_street = \"../input/intel-image-classification/seg_test/seg_test/street/20070.jpg\"\n\nsamples = [sample_building, sample_forest, sample_glacier, sample_mountain, sample_sea, sample_street]\ntarget_sample = [0, 1, 2, 3, 4, 5]\nfrom torchvision.io import read_image\nfor i in range(len(samples)):\n    target_cpu = torch.Tensor([target_sample[i]]).type(torch.LongTensor)\n    target = target_cpu.to(device_use)\n    sample = samples[i]\n    img = Image.open(sample).convert(\"RGB\")\n\n    img_show = img_transform(img)\n    \n    img_t = attention_transform(img)\n    img_input_cpu = Variable(img_t, requires_grad=True).unsqueeze(dim=0)\n    img_input = img_input_cpu.to(device_use)\n    model_out = model(img_input)\n    img_input.retain_grad()\n    print(model_out)\n    loss = model_out[0][i]\n    loss.backward()\n    images_grads = img_input.grad.data\n    abs_images_grads = images_grads.abs()\n    saliency, _ = abs_images_grads.max(dim=1)\n    \n    saliency = saliency.cpu().numpy()\n    N = img_input.shape[0]\n    for i in range(N):\n        plt.subplot(2, N, i + 1)\n        plt.imshow(img_show[i])\n        plt.axis('off')\n        plt.subplot(2, N, N + i + 1)\n        plt.imshow(saliency[i], cmap=plt.cm.hot)\n        plt.axis('off')\n        plt.gcf().set_size_inches(12, 5)\n    plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-16T14:46:57.030267Z","iopub.execute_input":"2022-04-16T14:46:57.030528Z","iopub.status.idle":"2022-04-16T14:47:04.142789Z","shell.execute_reply.started":"2022-04-16T14:46:57.0305Z","shell.execute_reply":"2022-04-16T14:47:04.140365Z"},"trusted":true},"execution_count":null,"outputs":[]}]}